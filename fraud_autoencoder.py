# -*- coding: utf-8 -*-
"""Fraud_Autoencoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ubwsriaIhYtaohRbcEHqrdmi9urVb7dL

## **Fraud detection**

**Anomaly detection in credit cards transactions (frauds) using unsupervised deep learning model - Autoencoder.**

The aim of this project is predict whether a given transaction was a fraud or not. In this case we will apply anomaly detection with Autoencoder - unsupervised deep learning model. For our analysis we will use the Python, mostly Tensorflow Keras library to illustrate the process of identifying outliers using an autoencoder.

**Dataset:**

The dataset used for credit cards fraud detection is derived from Kaggle.

It contains transactions made by credit cards in two days in 2013 by European cardholders. There are 492 frauds out of 284,807 transactions.

### **Import libriaries and data**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras import layers, losses
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(tf.__version__)

df = pd.read_csv('/content/sample_data/creditcard.csv')
df.head()

"""**First observations:**"""

df.shape

df.info()

"""Checking missing values:"""

df.isnull().sum()

"""The dataset consists only numerical values and majority features (V1 to V28) are the result of ‘Principal Component Analysis (PCA)’ transformation. The ‘Time’ and ‘Amount’ features are not transformed data. There are no missing values in the dataset.

### **Data preparation**

**Data normalization**

The majority of our data are normalized (V1 to V28 values) which are result of PCA analysis. We also have to normalized rest of the data (Time and Amount) so that their high values do not adversely affect the training of the model.
"""

df_sc = df.copy()

df_sc['Time'] = StandardScaler().fit_transform(df_sc['Time'].values.reshape(-1, 1))
df_sc['Amount'] = StandardScaler().fit_transform(df_sc['Amount'].values.reshape(-1, 1))

"""**Splitting dataset into train and test data**

We prepare our data assumming that frauds are anomalies in our transactions. To better build anomaly detection model we create train data with only normal transactions without fraud. The frauds go into test data only.

"""

train, test = train_test_split(df_sc, test_size=0.3, random_state = 10)

X_train = train[train['Class'] == 0]
X_train = X_train.drop(['Class'], axis=1)

X_test = test.drop(['Class'], axis=1)
y_test = test['Class']

print(f'X_train shape: {X_train.shape};')
print(f'X_test shape: {X_test.shape}; y_test shape: {y_test.shape}')

"""### **Build model**

We build autoencoder model for anomaly detection.

The Autoencoder is a neural network which learns to copy its inputs to outputs, i.e. it learn the compressed representation of raw data. It is based on unsupervised machine learning that applies the backpropagation technique and sets the target values equal to the inputs. Autoencoder contains two parts named encoder and decoder. The encoder compresses the input into a latent space representation while the decoder decodes the encoded image back to the original image of the same dimension.

In our case we used the architecture for deep autoencoders, we reduce the amount of input data to a smaller representation of neurons.
We create the input layer, encoder layers and decoder layers. In the input layer, we specifie the shape of the dataset (dataset has 30 features so the shape is 30). First three layers are used for our encoder and consists of layers with 16, 8, and 4 neurons, respectively. The last three layers are the decoder consisting of layers with 8, 16, and 30 neurons, respectively. Additionally, L1 regularization will be used during training.
After defining the input, encoder and decoder layers, we create the autoencoder model to combine the layers.
"""

learning_rate = 0.00001
input_dim = X_train.shape[1]

input_layer = Input(shape=(input_dim, ))

encoder = Dense(16, activation='elu', activity_regularizer=regularizers.l1(learning_rate))(input_layer)
encoder = Dense(8, activation='relu')(encoder)
encoder = Dense(4, activation='relu')(encoder)

decoder = Dense(8, activation='relu')(encoder)
decoder = Dense(16, activation='relu')(decoder)
decoder = Dense(input_dim, activation='elu')(decoder)

autoencoder = Model(inputs=input_layer, outputs=decoder)

"""We compile the model with the optimizer of adam and the loss of mse (Mean Squared Error). We also define an early stopping method to avoid training longer than needed."""

autoencoder.compile(optimizer='adam',
                    metrics=['accuracy'],
                    loss='mean_squared_error')


EarlyStop = EarlyStopping(monitor='accuracy', patience=5, verbose=1)

autoencoder.summary()

"""We train the model for 20 epoch:"""

history = autoencoder.fit(X_train, X_train,
          epochs=20,
          batch_size = 64,
          validation_data = (X_test, X_test),
          callbacks = EarlyStop,
          shuffle=True)

"""Visualization of training:"""

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

"""This charts show the training and validation loss and accuracy changes during the model fitting. We can see that both training and validation losses decrease with the increase of epochs.

### **Model evaluation**

In this step we use our model to predict the outliers. We use `predict` method to  get the reconstruction value for the testing data set containing the usual data points and the outliers.
"""

prediction = autoencoder.predict(X_test)

"""Now we look at the error distribution:"""

mse = np.mean(np.power(X_test - prediction, 2), axis=1)
error_df = pd.DataFrame({'reconstruction_error': mse, 'class': y_test})

error_df.describe()

"""Reconstruction error histogram on test sample:"""

bins = 50
plt.figure(figsize=(12, 6))
plt.hist(error_df[error_df['class'] == 0]['reconstruction_error'], bins, alpha=1, density=True, label='Normal')
plt.hist(error_df[error_df['class'] == 1]['reconstruction_error'], bins, alpha=0.6, density=True, label='Fraud', color='crimson')
plt.legend(loc='upper right')
plt.title("Reconstruction error histogram on test sample")
plt.xlabel("Reconstruction error")
plt.ylabel("Percentage of transactions (%)");
plt.show()

"""From chart above we can see that most of the transactions with a high spread are actually frauds.


**Reconstruction Error vs Threshold Check**

In order to predict whether or not a new/unseen transaction is normal or fraud, we’ll calculate the reconstruction error from the transaction data itself. We define a threshold based on which
we will predicted normal data points. If the error is larger than a  threshold, we’ll mark it as a fraud (since our model should have a low error on normal transactions). In our case the threshold is 3.
"""

threshold=3

groups = error_df.groupby('class')
fig, ax = plt.subplots(figsize=(12, 6))

for name, group in groups:
    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',
            label= "Fraud" if name == 1 else "Normal")
ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors="r", zorder=100, label='Threshold')
ax.legend()
plt.title("Reconstruction error for different classes")
plt.ylabel("Reconstruction error")
plt.xlabel("Data point index")
plt.show();

"""We see that our model  seems to catch a lot of the fraud cases. We can also increase or decrease the value of the threshold, depending on the problem.


We also check the prediction performance:
"""

threshold_prediction = [0 if i < threshold else 1 for i in mse]

print(classification_report(y_test, threshold_prediction))

"""The recall value of 0.83 shows that around 83% of the outliers were captured by the autoencoder.

**Confusion Matrix**
"""

y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]
conf_matrix = confusion_matrix(error_df['class'], y_pred)

plt.figure(figsize=(7, 5))
sns.heatmap(conf_matrix, xticklabels=['Normal','Fraud'],
            yticklabels=['Normal','Fraud'],
            annot=True, fmt="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()

"""Looking at the confusion matrix we see pretty low False Negative rate and 117 frauds correctly classified by the model. Only stayed 24 of fraud transactions not correctly classified by the model.

### **Summary**



This project was aimed to fraud detection in credit cards transactions. In this part we used  autoencoders to fraud anomaly detection. We built the model only on one-class examples (normal transactions) with no suspicious transactions. The architecture which we have used for deep autoencoders it seems to be good and catching fraud transactions.  We evaluated our model with a few methods such as reconstruction error,  recall and confusion matrix. Our model  captures 83% of the anomaly data points (based on recall value) and  has the low False Negatives rate and will be not miss many anomalies.
"""

# Create a new transaction with features
new_transaction = {
    'Time': 1000,
    'V1': 0.5,
    'V2': -0.3,
    # ... Include all feature values ...
    'V28': 0.01,
    'Amount': 100.0
}

# Manually set the 'Class' label to 1 for fraudulent or 0 for non-fraudulent
new_transaction['Class'] = 1  # Set to 1 for a fraudulent transaction

# Check if the new transaction is fraudulent or not
if new_transaction['Class'] == 1:
    print("The new transaction is fraudulent.")
else:
    print("The new transaction is not fraudulent.")